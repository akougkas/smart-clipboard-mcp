# Smart Clipboard MCP Server - Environment Configuration Template
# Copy this file to .env and configure your AI providers

# ============================================================================
# LOCAL AI SERVERS (Automatically detected if running)
# ============================================================================

# LM Studio (typically runs on port 1234)
LMSTUDIO_API_BASE=http://localhost:1234
# Your detected: http://192.168.86.20:1234

# Ollama (typically runs on port 11434)
OLLAMA_API_BASE=http://localhost:11434

# vLLM Server
VLLM_API_BASE=http://localhost:8000

# Text Generation WebUI
TEXTGEN_API_BASE=http://localhost:5000

# ============================================================================
# CLOUD AI PROVIDERS (Add any API keys you have)
# ============================================================================

# OpenAI (recommended for best embeddings)
OPENAI_API_KEY=
OPENAI_ORG_ID=

# Anthropic Claude
ANTHROPIC_API_KEY=

# Google AI / Vertex AI
GOOGLE_API_KEY=
VERTEXAI_PROJECT=
VERTEXAI_LOCATION=

# Azure OpenAI
AZURE_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=2024-02-01

# Cohere
COHERE_API_KEY=

# Together AI
TOGETHER_API_KEY=

# Groq (fast inference)
GROQ_API_KEY=

# Hugging Face
HUGGINGFACE_API_KEY=

# Mistral AI
MISTRAL_API_KEY=

# Perplexity
PERPLEXITYAI_API_KEY=

# Replicate
REPLICATE_API_TOKEN=

# ============================================================================
# MODEL PREFERENCES (Optional - will auto-select best available)
# ============================================================================

# Preferred models (will use if available, otherwise auto-select)
PREFERRED_EMBEDDING_MODEL=
PREFERRED_CHAT_MODEL=

# Model routing strategy: simple-shuffle, least-busy, usage-based-routing, latency-based-routing
ROUTING_STRATEGY=latency-based-routing

# ============================================================================
# CACHE CONFIGURATION
# ============================================================================

# Cache backend: auto, redis, memory
CACHE_BACKEND=auto

# Redis settings (if using Redis cache)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Cache TTL in seconds
EMBEDDING_CACHE_TTL=3600
CHAT_CACHE_TTL=300

# ============================================================================
# PERFORMANCE SETTINGS
# ============================================================================

# Request timeout in seconds
REQUEST_TIMEOUT=30

# Number of retries for failed requests
MAX_RETRIES=3

# Cooldown time for failed providers (seconds)
COOLDOWN_TIME=60

# Enable pre-call checks (validates context window before requests)
ENABLE_PRE_CALL_CHECKS=true

# ============================================================================
# AGENT MODE CONFIGURATION
# ============================================================================

# Auto-capture settings for AI agents
AGENT_AUTO_CAPTURE=true
AGENT_MIN_CONTENT_LENGTH=50
AGENT_MAX_CONTENT_LENGTH=10000

# Content filtering for agents
AGENT_FILTER_CODE=true
AGENT_FILTER_DOCS=true
AGENT_FILTER_CONFIGS=true

# ============================================================================
# STORAGE SETTINGS
# ============================================================================

# Local storage path for LanceDB
STORAGE_PATH=~/.smart-clipboard/lancedb

# Maximum clips to store
MAX_CLIPS=100000

# ============================================================================
# DEBUG & LOGGING
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable detailed AI provider logging
AI_DEBUG=false

# Enable performance metrics
ENABLE_METRICS=true